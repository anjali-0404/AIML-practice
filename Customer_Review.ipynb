{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN1ZDnrbiXHUjssKM0aS5DE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anjali-0404/AIML-practice/blob/main/Customer_Review.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gBoMJEvXSMe3",
        "outputId": "52cc63a2-483a-456b-cbfb-5f4f113ee079"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample data:\n",
            "    id  rating                                             review\n",
            "0   1       2   @user when a father is dysfunctional and is s...\n",
            "1   2       5  @user @user thanks for #lyft credit i can't us...\n",
            "2   3       2                                bihday your majesty\n",
            "3   4       5  #model   i love u take with u all the time in ...\n",
            "4   5       1             factsguide: society now    #motivation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 187ms/step - accuracy: 0.2022 - loss: 1.6106 - val_accuracy: 0.1989 - val_loss: 1.6115\n",
            "Epoch 2/3\n",
            "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 188ms/step - accuracy: 0.2020 - loss: 1.6104 - val_accuracy: 0.1981 - val_loss: 1.6096\n",
            "Epoch 3/3\n",
            "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 184ms/step - accuracy: 0.2032 - loss: 1.6096 - val_accuracy: 0.1989 - val_loss: 1.6097\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 45ms/step\n",
            "\n",
            "Rating Prediction Accuracy: 0.19959330517753793\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00      1278\n",
            "           1       0.00      0.00      0.00      1277\n",
            "           2       0.20      1.00      0.33      1276\n",
            "           3       0.00      0.00      0.00      1282\n",
            "           4       0.00      0.00      0.00      1280\n",
            "\n",
            "    accuracy                           0.20      6393\n",
            "   macro avg       0.04      0.20      0.07      6393\n",
            "weighted avg       0.04      0.20      0.07      6393\n",
            "\n",
            "\n",
            "Sentiment Accuracy: 0.5992491787893008\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.60      1.00      0.75      3831\n",
            "     Neutral       0.00      0.00      0.00      1282\n",
            "    Positive       0.00      0.00      0.00      1280\n",
            "\n",
            "    accuracy                           0.60      6393\n",
            "   macro avg       0.20      0.33      0.25      6393\n",
            "weighted avg       0.36      0.60      0.45      6393\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
            "\n",
            "🗣️ Review: The product quality is excellent and delivery was quick!\n",
            "⭐ Predicted Rating: 3\n",
            "🙂 Predicted Sentiment: Neutral\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "# ===============================\n",
        "# 📦 Import Required Libraries\n",
        "# ===============================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# ===============================\n",
        "# 📥 Load Predefined Dataset\n",
        "# ===============================\n",
        "# Dataset source: Amazon reviews sample (small size for demo)\n",
        "url = \"https://raw.githubusercontent.com/dD2405/Twitter_Sentiment_Analysis/master/train.csv\"\n",
        "df = pd.read_csv(url)\n",
        "df = df.rename(columns={'tweet': 'review', 'label': 'rating'})  # Rename for uniformity\n",
        "\n",
        "# Simulate 1–5 star ratings (for demonstration)\n",
        "# Original dataset has 0 (negative), 1 (positive)\n",
        "df['rating'] = df['rating'].apply(lambda x: np.random.randint(1, 6))\n",
        "\n",
        "print(\"Sample data:\\n\", df.head())\n",
        "\n",
        "# ===============================\n",
        "# 🧹 Clean and Preprocess Text\n",
        "# ===============================\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "df['clean_review'] = df['review'].apply(clean_text)\n",
        "\n",
        "# ===============================\n",
        "# 💬 Create Sentiment Label\n",
        "# ===============================\n",
        "def get_sentiment(rating):\n",
        "    if rating <= 2:\n",
        "        return \"Negative\"\n",
        "    elif rating == 3:\n",
        "        return \"Neutral\"\n",
        "    else:\n",
        "        return \"Positive\"\n",
        "\n",
        "df['sentiment'] = df['rating'].apply(get_sentiment)\n",
        "\n",
        "# ===============================\n",
        "# 🔤 Tokenization and Padding\n",
        "# ===============================\n",
        "vocab_size = 10000\n",
        "max_len = 100\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(df['clean_review'])\n",
        "\n",
        "X = tokenizer.texts_to_sequences(df['clean_review'])\n",
        "X = pad_sequences(X, maxlen=max_len, padding='post', truncating='post')\n",
        "\n",
        "# Encode ratings and sentiments\n",
        "label_encoder_rating = LabelEncoder()\n",
        "label_encoder_sentiment = LabelEncoder()\n",
        "\n",
        "y_rating = label_encoder_rating.fit_transform(df['rating'])\n",
        "y_sentiment = label_encoder_sentiment.fit_transform(df['sentiment'])\n",
        "\n",
        "# ===============================\n",
        "# 🧩 Train-Test Split\n",
        "# ===============================\n",
        "X_train, X_test, y_rating_train, y_rating_test, y_sent_train, y_sent_test = train_test_split(\n",
        "    X, y_rating, y_sentiment, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# ===============================\n",
        "# 🧠 Build LSTM Model\n",
        "# ===============================\n",
        "embedding_dim = 64\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, input_length=max_len),\n",
        "    LSTM(128, return_sequences=False),\n",
        "    Dropout(0.4),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(5, activation='softmax')  # For 1–5 rating prediction\n",
        "])\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# ===============================\n",
        "# 🚀 Train Model\n",
        "# ===============================\n",
        "history = model.fit(X_train, y_rating_train, epochs=3, batch_size=64, validation_split=0.2)\n",
        "\n",
        "# ===============================\n",
        "# 🎯 Evaluate Model\n",
        "# ===============================\n",
        "pred_rating = np.argmax(model.predict(X_test), axis=1)\n",
        "print(\"\\nRating Prediction Accuracy:\", accuracy_score(y_rating_test, pred_rating))\n",
        "print(classification_report(y_rating_test, pred_rating))\n",
        "\n",
        "# ===============================\n",
        "# 💬 Sentiment Prediction (Derived)\n",
        "# ===============================\n",
        "pred_sentiment = ['Positive' if r >= 4 else 'Neutral' if r == 3 else 'Negative' for r in pred_rating]\n",
        "true_sentiment = ['Positive' if r >= 4 else 'Neutral' if r == 3 else 'Negative' for r in y_rating_test]\n",
        "\n",
        "print(\"\\nSentiment Accuracy:\", accuracy_score(true_sentiment, pred_sentiment))\n",
        "print(classification_report(true_sentiment, pred_sentiment))\n",
        "\n",
        "# ===============================\n",
        "# 🧾 Test with Custom Review\n",
        "# ===============================\n",
        "test_review = [\"The product quality is excellent and delivery was quick!\"]\n",
        "seq = tokenizer.texts_to_sequences(test_review)\n",
        "padded = pad_sequences(seq, maxlen=max_len, padding='post')\n",
        "pred = np.argmax(model.predict(padded))\n",
        "pred_rating_label = label_encoder_rating.inverse_transform([pred])[0]\n",
        "pred_sent_label = get_sentiment(pred_rating_label)\n",
        "\n",
        "print(\"\\n🗣️ Review:\", test_review[0])\n",
        "print(\"⭐ Predicted Rating:\", pred_rating_label)\n",
        "print(\"🙂 Predicted Sentiment:\", pred_sent_label)\n"
      ]
    }
  ]
}